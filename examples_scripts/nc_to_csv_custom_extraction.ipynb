{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melamedturkishk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xarray\\core\\merge.py:10: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)\n"
     ]
    }
   ],
   "source": [
    "# This script will read in the NetCDF files from a custom extraction (provincial) \n",
    "# and convert to CSV files for each grid cell\n",
    "\n",
    "# This will clear all the variables before runnnig the code\n",
    "for name in dir():\n",
    "    if not name.startswith('_'):\n",
    "        del globals()[name]\n",
    "\n",
    "# Import required packages\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from netCDF4 import num2date\n",
    "\n",
    "# Load in the NetCDF files for each RCP. This could be automated a bit better in terms of the \n",
    "# filenames and variable names, but works well for now.\n",
    "var_rcp26_ds = xr.open_dataset('/Users/melamedturkishk/Desktop/Kai/Custom/QuebecBCCAQv2+ANUSPLIN300_ensemble-percentiles_historical+rcp26_1950-2100_gddgrow_0_YS.nc')\n",
    "var_rcp45_ds = xr.open_dataset('/Users/melamedturkishk/Desktop/Kai/Custom/QuebecBCCAQv2+ANUSPLIN300_ensemble-percentiles_historical+rcp45_1950-2100_gddgrow_0_YS.nc')\n",
    "var_rcp85_ds = xr.open_dataset('/Users/melamedturkishk/Desktop/Kai/Custom/QuebecBCCAQv2+ANUSPLIN300_ensemble-percentiles_historical+rcp85_1950-2100_gddgrow_0_YS.nc')\n",
    "\n",
    "# Extract the time, lat, and lon data\n",
    "time = np.asarray(var_rcp26_ds.time)\n",
    "lat = np.asarray(var_rcp26_ds.lat)\n",
    "lon = np.asarray(var_rcp26_ds.lon)\n",
    "\n",
    "# Extract the 10th, 50th and 90th percentile data from each RCP. This can be something that's automated in\n",
    "# a more slick way so you don't have to change the arrays we're extracting from, but for now this works fine\n",
    "# RCP 2.6\n",
    "var_p10_rcp26 = var_rcp26_ds.gddgrow_0_p10.data\n",
    "var_p50_rcp26 = var_rcp26_ds.gddgrow_0_p50.data\n",
    "var_p90_rcp26 = var_rcp26_ds.gddgrow_0_p90.data\n",
    "\n",
    "# RCP 4.5\n",
    "var_p10_rcp45 = var_rcp45_ds.gddgrow_0_p10.data\n",
    "var_p50_rcp45 = var_rcp45_ds.gddgrow_0_p50.data\n",
    "var_p90_rcp45 = var_rcp45_ds.gddgrow_0_p90.data\n",
    "\n",
    "# RCP 8.5\n",
    "var_p10_rcp85 = var_rcp85_ds.gddgrow_0_p10.data\n",
    "var_p50_rcp85 = var_rcp85_ds.gddgrow_0_p50.data\n",
    "var_p90_rcp85 = var_rcp85_ds.gddgrow_0_p90.data\n",
    "\n",
    "# Extract the 10th, 50th, and 90th percentile time series for each grid point, concatenate together, then save as CSV\n",
    "# Pre-allocate based on number of columns (10th percentile, 50th percentile, and 90th percentile for each RCP), \n",
    "# rows (years from 1950-2100) and lat and lon (3rd and 4th dimensions)\n",
    "var_lat_lon_prcnt = np.zeros((var_p10_rcp26.shape[0], 9, var_p10_rcp26.shape[1], var_p10_rcp26.shape[2]))\n",
    "\n",
    "# Pre-allocate a lat, lon variable that will provide the lat-lon coordinates corresponding to each grid point tme series\n",
    "lat_lon_rep = np.zeros((var_p10_rcp26.shape[0], 2, var_p10_rcp26.shape[1], var_p10_rcp26.shape[2]))\n",
    "\n",
    "for lt in range(var_p10_rcp26.shape[1]): \n",
    "    for ln in range(var_p10_rcp26.shape[2]):\n",
    "        var_lat_lon_prcnt[:, :, lt, ln] = np.transpose([np.asarray(var_p10_rcp26[:, lt, ln]), \n",
    "                                                                    np.asarray(var_p50_rcp26[:, lt, ln]),\n",
    "                                                                    np.asarray(var_p90_rcp26[:, lt, ln]), \n",
    "                                                                    np.asarray(var_p10_rcp45[:, lt, ln]), \n",
    "                                                                    np.asarray(var_p50_rcp45[:, lt, ln]),\n",
    "                                                                    np.asarray(var_p90_rcp45[:, lt, ln]), \n",
    "                                                                    np.asarray(var_p10_rcp85[:, lt, ln]), \n",
    "                                                                    np.asarray(var_p50_rcp85[:, lt, ln]),\n",
    "                                                                    np.asarray(var_p90_rcp85[:, lt, ln])])\n",
    "        lat_lon_rep[:, :, lt, ln] = np.transpose(np.squeeze(np.asarray([np.asarray([np.repeat(lat[lt], var_p10_rcp26.shape[0])]), \n",
    "                                                  np.asarray([np.repeat(lon[ln], var_p10_rcp26.shape[0])])])))\n",
    "        \n",
    "# Convert to nparray\n",
    "var_lat_lon_prcnt = np.asarray(var_lat_lon_prcnt)\n",
    "lat_lon_rep = np.asarray(lat_lon_rep)\n",
    "\n",
    "# Rearrange the dimensions and reshape the array so that the third dimension concatenates into the rows\n",
    "var_lat_lon_prcnt_all = var_lat_lon_prcnt.transpose(3, 0, 1, 2).reshape(var_p10_rcp26.shape[0]*var_p10_rcp26.shape[2],\n",
    "                                                                                                9, var_p10_rcp26.shape[1])\n",
    "lat_lon_rep = lat_lon_rep.transpose(3, 0, 1, 2).reshape(var_p10_rcp26.shape[0]*var_p10_rcp26.shape[2],\n",
    "                                                                                                2, var_p10_rcp26.shape[1])\n",
    "#print(precip_tot_time_lat_lon_prcnt_all.shape)\n",
    "\n",
    "# Repeat the same as above but for what was the fourth dimension, and is now the third\n",
    "var_lat_lon_prcnt_all = var_lat_lon_prcnt_all.transpose(2, 0, 1).reshape(var_p10_rcp26.shape[0]*var_p10_rcp26.shape[2]*var_p10_rcp26.shape[1], \n",
    "                                                                                                 9)\n",
    "lat_lon_rep = lat_lon_rep.transpose(2, 0, 1).reshape(var_p10_rcp26.shape[0]*var_p10_rcp26.shape[2]*var_p10_rcp26.shape[1], \n",
    "                                                                                                 2)\n",
    "\n",
    "# Add in a column for the year that repeats for each grid point\n",
    "year_rep = np.expand_dims(np.tile(np.arange(1950, 2101, 1), var_p10_rcp26.shape[1]*var_p10_rcp26.shape[2]), \n",
    "                          axis=1)\n",
    "\n",
    "# Add/concatenate in the year, and the lat and lon to the actual time series data, for reference\n",
    "var_time_lat_lon_prcnt_all = np.hstack((year_rep, lat_lon_rep, var_lat_lon_prcnt_all))\n",
    "\n",
    "# Useful to see the final shape of the array you will save in order to figure out how many files to save\n",
    "# Row limit in Excel: 1,048,576, Column limit in Excel: 16,384\n",
    "#print(var_time_lat_lon_prcnt_all.shape)\n",
    "\n",
    "# Finally, add in headers for each of the columns\n",
    "headers = np.array([\"Date\", \"Latitude\", \"Longitude\", \"RCP_2.6_10th_percentile\", \"RCP_2.6_50th_percentile\", \"RCP_2.6_90th_percentile\", \n",
    "           \"RCP_4.5_10th_percentile\", \"RCP_4.5_50th_percentile\", \"RCP_4.5_90th_percentile\", \"RCP_8.5_10th_percentile\", \n",
    "           \"RCP_8.5_50th_percentile\", \"RCP_4.5_90th_percentile\"], dtype = object)\n",
    "\n",
    "var_with_headers = np.vstack((headers, np.asarray(var_time_lat_lon_prcnt_all, dtype = object)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping this last bit of code in a separate cell because the precious cell takes longer to run, and can then\n",
    "# simply run this last bit where we save the different CSV files multiple times until we get all the data\n",
    "# This last bit of code could also probably still be automated in a better way, but for now, manually\n",
    "# saving the data into ten separate CSV files because of row limit in Excel\n",
    "# Save file as CSV with format (fmt) as string because of headers\n",
    "np.savetxt(\"/Users/melamedturkishk/Desktop/Kai/Custom/deg_days_0_annual_ensemble_rcps_prcntiles_QC_10.csv\", \n",
    "           np.vstack((headers, np.asarray(var_time_lat_lon_prcnt_all[1020005*9::, :], dtype = object))), delimiter = \" \", fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
